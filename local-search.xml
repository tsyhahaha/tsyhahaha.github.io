<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>线性分类器</title>
    <link href="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <url>/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="线性分类器（Linear-Classification）–-score-and-loss"><a href="#线性分类器（Linear-Classification）–-score-and-loss" class="headerlink" title="线性分类器（Linear Classification）– score and loss"></a>线性分类器（Linear Classification）– score and loss</h2><p><a href="http://tsyhahaha.info/index.php/author/tsyhahaha/">By tsyhahaha</a> <a href="http://tsyhahaha.info/1970/01">2022年2月10日</a> <a href="http://tsyhahaha.info/index.php/2022/02/10/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8linear-classification1/#respond">0</a></p><p>本章开始要逐步向神经网络靠近。作为第二个分类器“线性分类器”，比kNN强大得多，凭借<strong>score function</strong>（评分函数）得出测试集元素在各标签下的得分，凭借<strong>loss function</strong>（损失函数）来评判效果从而进行有目的的<strong>optimization</strong>（优化）。使用<strong>parametric approach</strong>（参数化方法），最终利用训练好的一个矩阵和一个向量，对测试数据进行矩阵乘法加法即可得出结果。可以说，这一节不论是逻辑性还是目的性，都要上升一个档次。</p><h2 id="Score-function"><a href="#Score-function" class="headerlink" title="Score function"></a>Score function</h2><p>认识线性分类器，首先就是要明确它的分类依据，而这个依据正是得分。结合“线性”，我们定义最简单的线性分类器的score function：<br>$$<br>f(x_i,W,b)&#x3D;Wx_i<br>$$<br>其中，W是权重（weights），xixi是输入的测试元素（向量），结果即为score。</p><p>更细致一点来讲：根据第一节，xixi事实上是一个行向量，但是在这里我们把它定义为一个<strong>列向量</strong>。W的行数的所有互不相同的label总数，比如若只有猫、狗、船三类，W行数就等于3；列数对应于xixi的维数（从矩阵乘法容易得知）。事实上在实际应用中我们还会加上一个偏移量b（bias）<br>$$<br>f(x_i,W,b)&#x3D;Wx_i+b<br>$$</p><p>最后结果就是一个列向量，每一行包含一个数，即为对应label的得分。见下例：</p><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/QQ%E5%9B%BE%E7%89%8720220210113851-1024x353.jpg" alt="img"></p><p>显然得分最高的即为预测结果，但是上图中，cat score却为负数，这就说明W和b错的很离谱，因此才需要不断的优化（optimization），最后才能得出正确率较高的结果。</p><p><strong>进一步理解</strong>：其实W的每一行，是隶属于一个label的，从上图的颜色就可以看出；而行内的每个元素对应的是测试元素每个像素（pixel）的weight。所以更新W其实就是更新每个像素的权重，从而尽可能得到对应每一个label的<strong>普适模板</strong>，然后进行score的计算。事实上这样分析我们也很容易发现纰漏，训练是得到一个单一模板，但图片结构的变化往往会带来多个模板，最简单的例子如下：<img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source\_posts\image\R-C-1.jpg" width="500"></p><img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source\_posts\image\R-C.jpg" width="500"><p>如果训练集中大部分都是朝左的马头，但是测试集来了一个朝右的可能score就会下降很多，这也是线性分类器朝着多层的神经网络发展的原因之一。</p><p><strong>线性分类器的可视化：</strong>对于“线性分类器到底在干什么？”，我们希望能通过可视化来解释，但是事实上对于高维的对象，我们很难线性的画在图上，因此不妨将其以某种方式压缩到二维，来尽可能解释linear工作：</p><p><img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source_posts\image\QQ图片20220210115800-1024x732.jpg" alt="QQ图片20220210115800-1024x732"></p><p>上图中对于每个图片对应一个坐标，输入到线性分类器中，图中某条线作为一个分界，这个分界的某一侧得分较高，且离分界线越远越高；而另一侧得分就较低，且离分界线越远越低。于是起到了分类的作用。</p><p><strong>bias trick</strong>：认识了如何score function，我们接下来就要进行训练，显然训练的包括W和b，为了能简化训练，我们介绍bias trick。如图所示：</p><p><img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source_posts\image\QQ图片20220210133157-1024x417.jpg" alt="QQ图片20220210133157-1024x417"></p><p>其想法就是在xixi拓展一维，而增加的一维中的元素恒为1，这样就能把b合并到W中了。这样一来，二者就可以一同训练，表观上<strong>只需要训练W</strong>就行了。</p><h2 id="SVM-Loss-Function"><a href="#SVM-Loss-Function" class="headerlink" title="SVM Loss Function"></a>SVM Loss Function</h2><p>训练需要依据，在线性分类器中，我们选择的依据便是loss function（损失函数），或者称作cost function（代价函数）、objective（目标）。十分直观的是，作为“损失”，显然其值越大，W的取值越不好，我们期望不断训练W能让loss function收敛于最小值。</p><p><strong>1.SVM（Multiclass Support Vector Machine loss）</strong></p><p>有很多定义损失函数的方式，其中一种典型的loss function称为Multiclass Support Vector Machine loss（多类支持向量机损失函数），简称SVM。</p><p><img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source_posts\image\SVM.png" alt="SVM"></p><p><strong>基本思想</strong>：既然定义了score function，那么我们就应该定义怎样选出我们的预测结果，当然可能会问：难道不是选出得分最高的就行了吗？但是设想一下，<strong>你希望99.8和99.7这样的得分同时出现吗？</strong>这显然不利于我们训练，也与我们的思维有违。一个很自然的想法就是：最终预测结果的score不仅是最高，还必须比其他的至少高个十几二十。这就是SVM的关键之处。</p><p><img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source_posts\image\QQ图片20220210151903-1024x146.jpg" alt="QQ图片20220210151903-1024x146"></p><p>为了量化上面的思想，SVM设置了一个margin（差额） Δ，借此可以定义损失函数为：<strong>所有非正确score与正确score的差值之和</strong>。所谓“非正确”，就是<strong>比正确score大</strong>或者<strong>大于score−Δ</strong>的那些得分（上图delta起始段右侧）。转化为公式如下：<br>$$<br>L_i&#x3D;\sum_{j\neq y_i}max(0,s_j-s_{y_i}+\Delta)<br>$$<br>或展开如下：<br>$$<br>L_i&#x3D;\sum_{j\neq y_i}max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)<br>$$<br>注：这里的yi指的是第i个训练数据对应的label序号，这里计算的Li对应的只是关于第i个训练数据的损失值。所以，总共有train_num个LiLi损失值。第二个公式不过是将得分的原始公式替换，wj表示的是W的第j行。</p><blockquote><p>Ex：if s&#x3D;[13,−7,11], yi&#x3D;0, Δ&#x3D;10, what is the loss?<br>$$<br>s&#x3D;[13,−7,11],yi&#x3D;0,Δ&#x3D;10\ so \ \ the\ \ result\ \ is:Li&#x3D;max(0,−7−13+10)+max(0,11−13+10)&#x3D;8<br>$$</p></blockquote><p>值得一提的是，SVM另外一个有趣的名字叫做<strong>hinge loss SVM</strong>（折页损失），这是因为max(0,x)函数图像就像一个正在翻页的书。SVM还衍生出了L2-SVM其损失函数是用$max(0,x)^2$定义的，所以见到了也不要大惊小怪。有时候L2的版本表现要比SVM好，这也是一个超参数的范畴，类似L1距离和L2距离。</p><p><strong>2.Regularization（正则化）：SVM完全体</strong></p><p>基于上述算法，我们容易得出，如果W是一个最优解（loss&#x3D;0），那么2W，3W……都是满足loss&#x3D;0的W，那这个时候我们应该怎么选择？基于<a href="http://baike.baidu.com/item/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80%E5%8E%9F%E7%90%86">奥卡姆剃刀原理</a>，我们希望从众多符合条件的W选取最为简洁的一个。因此，对于更复杂的W，我们要增加一个<strong>regularization（正则项）</strong>，目的为：如果让当前W替换成更复杂的W，那么loss function会增大，这是一个对“复杂化”的惩罚。</p><p><img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source_posts\image\OIP-C.jpg" alt="OIP-C"></p><p>可以看出过拟合与模型复杂度之间的关系，我们用L2范数来定义正则项，大家可以先结合上图自己思考一下为什么这样表征，后面会有例子解释：<br>$$<br>R(W)&#x3D;\sum_c\sum_l W_{c,l}^2<br>$$</p><p>最终<strong>loss function</strong>由两部分组成：所有<code>training data</code>的SVM损失值的平均+λ×正则项<br>$$<br>L&#x3D;\frac{1}{N}\sum_iL_i+\lambda R(W)<br>$$<br>或<br>$$<br>L&#x3D;\frac{1}{N}\sum_i\sum_{j\neq y_i}[max(0,f(x_i,W)<em>j-f(x_i,W)</em>{y_i}+\Delta)]+\lambda R(W)<br>$$<br><strong>进一步理解：</strong>到这里，虽然我们知道正则化是啥，如何操作，但是仍缺乏<strong>具象化认识</strong>，而这一步恰恰是掌握知识与别人不同之处。我们来看个例子：</p><blockquote><p>$$<br>x&#x3D;[1,1,1,1]^T, w1&#x3D;[1,0,0,0],\  w2&#x3D;[0.25,0.25,0.25,0.25], \Then\ w_1x&#x3D;w_2x&#x3D;1, but\ \ R1&#x3D;1&gt;R2&#x3D;0.25<br>$$</p></blockquote><p>这个例子告诉我们，加入正则项的loss更倾向于<strong>“分散的，取值较小的”</strong>W，这意味着选择后的W相比于其他的W要更加<strong>全面考虑输入量</strong>（因为分散），这也减小了<code>overfit</code>的极端可能性。</p><p><strong>3.Setting Delta Δ lambda λ</strong></p><p>正如你所预想，<code>delta</code>也是一个<code>hyperparameter</code>，或许我们也需要利用<code>cross validation</code>来选择。</p><p>但事实上，不必要，甚至你从头到尾都设置Δ&#x3D;1也无伤大雅。为什么这么说呢，这是因为Δ和λ所扮演的角色性质相似————权衡<code>data loss\Regularition loss</code>。如下图容易分辨：左侧为Δ大，或者λ小的情况，右侧则相反。</p><p><img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source\_posts\image\HsvdA2uJHB-1024x646.jpg" width="500"><img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source\_posts\image\6CEsDfKifc-1024x711.jpg" width="500"></p><p>因此出于简便考虑，我们一般不去考虑Δ的调整，而仅仅调整λ即可。</p><h2 id="Softmax-classifier"><a href="#Softmax-classifier" class="headerlink" title="Softmax classifier"></a>Softmax classifier</h2><p>SVM是两大流行分类器之一，另一个就是softmax classifier了。相较于SVM loss，softmax的loss function对人类来更直观，更容易理解。</p><p><strong>1.cross-entropy loss（交叉熵损失）</strong></p><p>出于直观，直接上图：<img src="/2022/04/07/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/MyBlog\tsyhahaha.github.io\source_posts\image\svmvssoftmax.png" alt="svmvssoftmax"></p><p>可以清楚的看到，Softmax是运用了hinge loss的结果fifi，先取指数进行归一化，然后取负对数作为损失函数：<br>$$<br>L_i&#x3D;-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})<br>$$<br>或者<br>$$<br>L_i&#x3D;-f_{y_i}+log(\sum_je^{f_j})<br>$$<br>上式中隐藏的$f_j(z)&#x3D;\frac{e_{z_j}}{\sum_ke^{z_k}}$也称为<code>softmax function</code>，这也是<code>softmax loss</code>名称的由来。至于<strong>什么是交叉熵，交叉熵用在了算法的什么地方</strong>，可以参考<a href="https://zhuanlan.zhihu.com/p/124309304">这篇文章</a>，推导与说明都很详细，此处不在赘述。最后不要忘了损失函数的完全形式：<br>$$<br>L&#x3D;\frac{1}{N}\sum_iL_i+\lambda R(W)<br>$$</p><p>最后，在实际应用时，softmax在自变量取值比较大的时候，往往会先将得到的score都减去最大值之后再取指数归一化，具体原理请参考<a href="https://zhuanlan.zhihu.com/p/29376573">这篇文章</a>。</p><p><strong>2.Comparison of SVM and softmax</strong></p><p>二者在运用的时，其实是相似的，不同的人对“哪个更好”抱有不同的看法。这里还是简要谈谈二者差异：</p><ul><li>process：通过上图，二者操作上的区别是显然的</li><li>locally and globally：SVM更关注<strong>局部的、绝对的</strong>数据性质，而softmax更关注<strong>全局的、相对的</strong>数据性质。举例而言，对于[10,9,9]和[10,-100,-100]这两组未归一化scores，SVM（假设delta&#x3D;1）loss均为0，但是softmax要进行对数归一化，二者结果迥然不同。</li></ul><h2 id="总结·Sum"><a href="#总结·Sum" class="headerlink" title="总结·Sum"></a>总结·Sum</h2><ul><li><strong>SVM</strong>：介绍了概念，可以看出，与kNN相比，SVM利用parametic approach极大浓缩了test的时间，这是设计思想上的极大进步</li><li><strong>score and loss function：</strong>介绍了评分函数和损失函数，明确SVM的思想，并利用可视化加深了理解</li><li><strong>Regularization：</strong>介绍了正则化，其实是对“模型复杂”的惩罚机制， 可以减少overfit、模型复杂程度</li><li><strong>bias trick：</strong>介绍了bias trick，在训练时可以将b并入W一起训练</li><li><strong>Regularization：</strong>通过正则化，来降低模型的overfit与复杂度</li><li><strong>ΔandλΔandλ：</strong>介绍了这两个超参数的等价性理解，实际上只需要关注λλ即可</li><li><strong>softmax：</strong>介绍了另一种linear classifier，应用时其实十分相似，不过利用了交叉熵，贴合人类认知，关注点与SVM有略微不同。</li></ul><p>PS：下一节将具体介绍optimization</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>最近邻分类器nnc与knn</title>
    <link href="/2022/04/07/%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB%E5%99%A8nnc%E4%B8%8Eknn/"/>
    <url>/2022/04/07/%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB%E5%99%A8nnc%E4%B8%8Eknn/</url>
    
    <content type="html"><![CDATA[<h1 id="最近邻分类器（NNC）与kNN"><a href="#最近邻分类器（NNC）与kNN" class="headerlink" title="最近邻分类器（NNC）与kNN"></a>最近邻分类器（NNC）与kNN</h1><p>作为最简单的分类器，最近邻分类器（Nearest Neighbor Classifier）与 kNN（k-Nearest Neighbor Classifier）在实际运用中并不常见，但它是体现数据驱动（data-driven approach）思想的最初算法。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>由于原理十分明了，我们直接根据其算法步骤来阐述其原理：</p><ul><li><strong>input</strong> <strong>and learning</strong>：输入training数据，并加以学习（训练）。对于此算法，所谓“学习”，仅仅是记录，或者说存起来。下面代码的行为：仅仅记录所有图片数据以及其标签（用序号代替）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, X, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Train the classifier. For k-nearest neighbors this is just </span><br><span class="hljs-string">    memorizing the training data.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - X: A numpy array of shape (num_train, D) containing the training data</span><br><span class="hljs-string">      consisting of num_train samples each of dimension D.</span><br><span class="hljs-string">    - y: A numpy array of shape (N,) containing the training labels, where</span><br><span class="hljs-string">         y[i] is the label for X[i].</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> self.X_train <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.y_train <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:   <span class="hljs-comment"># 如果还没有训练数据，则直接记录该数据</span><br>        self.X_train = X<br>        self.y_train = y.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:                                               <span class="hljs-comment"># 如果已有训练数据，则向后接入新的数据</span><br>            self.X_train = np.vstack((self.X_train, X))<br>            self.y_train = np.vstack((self.y_train, y.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)))<br></code></pre></td></tr></table></figure><ul><li><strong>evaluation</strong>：传入分类器从未见过的数据，使之加以预测。对于最近邻分类器来说，预测方法是将每一个输入的图片与已记录的本地图片进行“距离计算”，并排序，相邻最近的图片的 label 即为预测结果。</li></ul><p><strong>PS：</strong>X的shape为(num_train, D)，意思为每一行都是一个训练数据，功num_train个；而每个训练数据是D维行向量。所以对于图片来说，需要把其数据表示转化为这样一个行向量的形式，事实上cifar-10数据集已经做好了这个处理。</p><h2 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h2><p><strong>1.曼哈顿距离</strong></p><p>d1(I1,I2)&#x3D;∑p|Ip1−Ip2|d1(I1,I2)&#x3D;∑p|I1p−I2p|</p><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE-2021-10-11-145826-1024x285.png" alt="img"></p><p>简单来说，就是两个向量对应元素差的绝对值。曼哈顿距离也称作“城市街区距离”，这样一来就很容易理解了，意思就是只能通过横平竖直的街道行进，因此计算方法为“直接相减”。曼哈顿距离也记为L1距离（L1 distance）。</p><p><strong>2.欧几里得距离</strong></p><p>d1(I1,I2)&#x3D;∑p(Ip1−Ip2)2−−−−−−−−−−−√d1(I1,I2)&#x3D;∑p(I1p−I2p)2</p><p>学过线性代数对欧氏距离应当有一定的了解，其形式也容易理解。欧式距离也记作L2距离（L2 distance）。</p><p>定义了距离，就可以计算排序，得出训练集中最近邻元素了，而此元素的label即为预测结果。</p><p>计算距离有几个循环版本，L1不必说，L2最简单、速度最快的就是利用矩阵乘法的no-loop版本，其中X的定义见上面train的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_distances_no_loops</span>(<span class="hljs-params">self, X</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Compute the distance between each test point in X and each training point</span><br><span class="hljs-string">in self.X_train using no explicit loops.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Input / Output: Same as compute_distances_two_loops</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>    num_test = X.shape[<span class="hljs-number">0</span>]<br>    num_train = self.X_train.shape[<span class="hljs-number">0</span>]<br>    dists = np.zeros((num_test, num_train))<br>    <br>    a_2 = np.diagonal(np.dot(X, X.T)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    b_2 = np.diagonal(np.dot(self.X_train, self.X_train.T)).reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>    ab = np.dot(X, self.X_train.T)<br>    dists = (a_2 + b_2 - <span class="hljs-number">2</span>*ab) ** <span class="hljs-number">0.5</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;compute_distances_no_loop is done!&#x27;</span>)<br>    <br>    <span class="hljs-keyword">return</span> dists<br></code></pre></td></tr></table></figure><h2 id="kNN"><a href="#kNN" class="headerlink" title="kNN"></a>kNN</h2><p>完成了NNC仔细想，只取“最近邻”这单单一个元素，是否过于草率。根据我们的生活常识，答案是肯定的。通过下面两个图片很容易看出问题：背景为绿色调，而主题元素红色调偏多。这就极可能导致二者归为同一类，这也是实际应用中经常发生的。</p><figure><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/836c4a6f553843ebb4213449006b9112_800_0_max_jpg_92-2-768x1024.jpg" width="400"><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/159155097-1.jpg"></p><p>为了改善这类问题，我们不再选取单单第一个作为依据，而是排序之后选取前k个，然后将其进行<strong>出现次数数量统计</strong>，最多的作为我们预测的结果。</p><p>针对<strong>二维对象</strong>分类情景，下面是k分别选取1和5时的情况，显然库k&#x3D;1即为最近邻分类器。图中的点为所有的data，而不同颜色，是分类器所做分类工作的结果。其中，k&#x3D;1边缘噪音过于明显，图形很难说是连片分布；当k增至5时，就会发现各类边界要光滑的许多，最重要是“边缘突起”“内部混合”的现象大大改善。可见，增加k确实有改进作用。现在问题转化为，选取合适的k。</p><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/QQ%E5%9B%BE%E7%89%8720220209203557-1024x275.jpg" alt="img"></p><h2 id="超参数（hyperparameter）"><a href="#超参数（hyperparameter）" class="headerlink" title="超参数（hyperparameter）"></a>超参数（hyperparameter）</h2><p>所谓超参数，在机器学习中定义为需要人为提前设定的模型外部参数，不可自动学习；与之对应的是模型参数，是可以通过模型train而自动学习的参数。</p><p>上面的距离算法（L1 or L2）、k值的选取，以及之后神经网络的深度、学习率等等都是超参数。如何选取合适的超参数影响着模型的鲁棒性、计算量等等。超参数选的不好，会直接导致计算量激增，所训练的模型产生overfit（过拟合），预测准确度降低等问题。</p><p>PS：<strong>overfit</strong>（过拟合）：通过下图容易分辨出绿线（overfit）和黑线。过拟合不仅会导致模型复杂化（扭来扭去学习起来当然复杂），还会使得模型过度适合当前test dataset，当换一组test data其预测结果会大幅降低。或者这样理解，过拟合会将某些<strong>非典型的特征</strong>记录为<strong>必要特征</strong>，因此换一组有略微差异的测试集时，预测准确度就会降低。</p><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/v2-ef71817b8ae4485c767d98054e12c0da_r-2-1024x567.jpg" alt="img"></p><p><strong>那么如何选取超参数呢？</strong></p><p>最简单最鲁莽的方法，就是尽可能尝试超参数，在training data上训练后，在test data上检验结果，最后将超参数与测试结果可视化，从而选出最合适的超参数。这么干有几个明显的问题：</p><ul><li>计算量大大增加。相当于每选一次超参数都要从头训练一次模型，先不说时间，内存都不一定够。</li><li>可能会导致关于test data的overfit。因为是完全按照test data上测试的效果，换一组也许就不一样。</li><li>应用具有局限性。由于这种选取方法，需要不停迭代test data，没有test data就无法进行，而现实中往往需要拿出一个训练好的模型，对test进行一次性测试，因此缺乏灵活性。</li></ul><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/QQ%E5%9B%BE%E7%89%8720220209171252-1-1024x69.png" alt="img"></p><p><strong>Holdout交叉验证：</strong>为了改善，另外一个想法是<strong>从training data中分出一部分</strong>称为Validation来取代需要迭代的test data。也就是说，与前面的想法类似，但却是拿在Validation上的测试结果作为依据来选取。这样可以做到training和test相互独立，有一定参考意义。一般选取样本的20%~30%作为Validation，但也可能根据需要调整：</p><ul><li>Validation占比过低，training data多，会导致关于验证集的overfit</li><li>Validation占比过高，training data少，会导致欠拟合</li></ul><p>Holdout交叉验证是最简单的交叉验证方法，适合大量数据快速验证的情况。</p><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/QQ%E5%9B%BE%E7%89%8720220210093429.png" alt="img">右图是十种train&#x2F;Validation划分，对应的训练后的收敛结果。（图片来源：<a href="https://zhuanlan.zhihu.com/p/24825503">【机器学习】Cross-Validation（交叉验证）详解 - 知乎 (zhihu.com)</a>）</p><p><strong>k-fold交叉验证：</strong>根据前面说到Holdout的问题，容易想到，为何不让Validation的选取随机化，于是产生了k-fold（k折）交叉验证。具体做法是：</p><ul><li>将原始train data平均划分为k个数据子集（fold）</li><li>每次随机但不重复的选取一个fold作为Validation，计算出正确率（或者其他衡量标准，如错误率等）</li><li>将第二步所得的k个结果进行平均，作为最终结果</li></ul><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/QQ%E5%9B%BE%E7%89%8720220209211009-1024x181.png" alt="img"></p><p>容易看出，k也是一个超参数，但一般我们选取3、5或10。k-fold交叉验证可以很好的增加训练的泛化性，提高对training数据的利用效果。但一般来说，由于其计算量不小，因此适用于数据量较小的情况。下面是k&#x3D;5时数据可视化：</p><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/QQ%E5%9B%BE%E7%89%8720220210094435.png" alt="img"></p><h2 id="算法评价"><a href="#算法评价" class="headerlink" title="算法评价"></a>算法评价</h2><p>与自然规律一致，作为最原始的东西，必然有其局限性：</p><ul><li><strong>test time</strong>：我们所希望的分类器，绝对不是train时简单，而test的时候要花费大量时间；相反，我们希望test是一瞬间，而train时间长点无所谓。所谓“磨刀不误砍柴工”，这也是后面算法革新的关键。</li><li><strong>space cost</strong>：空间开销取决于data的量级，不仅要存储所有的training data，还要处理大量test data，这导致空间开销极其不稳定。</li><li><strong>algorithm</strong>：这类算法的关键点在于“距离”的定义，但是事实上距离是一个<strong>“非理性”的整体绝对概念</strong>，与人类的善于比较的<strong>相对局部思想</strong>相差甚远，或许同一张图片，换了个色调，二者“距离”就会大大增加；又或者，大相径庭的图片由于<strong>距离的整体性质</strong>却判为距离相近（如下图所示）。因此会产生很多<strong>反直觉的判断</strong>，尤其是在对待图片这种高维对象。</li></ul><p><img src="http://tsyhahaha.info/wp-content/uploads/2022/02/QQ%E5%9B%BE%E7%89%8720220210100548-1024x412.jpg" alt="img"></p><h2 id="总结·Sum"><a href="#总结·Sum" class="headerlink" title="总结·Sum"></a>总结·Sum</h2><ul><li><strong>最近邻分类器原理</strong>，记录训练数据，计算测试数据与训练数据的距离，比较选出最近测试数据label作为估计值。</li><li><strong>距离的定义</strong>，L1与L2距离的提出与计算。</li><li><strong>kNN</strong>，选出最近的k个而不是1个，是最近邻算法的简单改进。</li><li><strong>超参数</strong>，人为设定，不可学习，选取合适可以减小overfit的可能。通过Holdout交叉验证、k-fold交叉验证选取合适的超参数。</li><li><strong>算法评价</strong>，需要更贴近人类思维的算法。</li></ul></figure>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/04/06/hello-world/"/>
    <url>/2022/04/06/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
